{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sharm\\phi-2-fine-tuning\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    GenerationConfig\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from huggingface_hub import interpreter_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since neil-code/dialogsum-test couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at C:\\Users\\sharm\\.cache\\huggingface\\datasets\\neil-code___dialogsum-test\\default\\0.0.0\\f0524dd2e0267dc8102109ce1b14bae8f97976d3 (last modified on Thu Oct 17 16:08:44 2024).\n"
     ]
    }
   ],
   "source": [
    "hugging_face_dataset = \"neil-code/dialogsum-test\"\n",
    "dataset = load_dataset(hugging_face_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'train_0',\n",
       " 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\",\n",
       " 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\",\n",
       " 'topic': 'get a check-up'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['load_in_8bit_fp32_cpu_offload']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    }
   ],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    load_in_8bit_fp32_cpu_offload=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.57s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'microsoft/phi-2'\n",
    "device_map = {\"\":0}\n",
    "original_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device_map,\n",
    "    quantization_config = bnb_config,\n",
    "    token = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code = True,\n",
    "    padding_side = \"left\",\n",
    "    add_eos_token = True,\n",
    "    add_bos_token = True,\n",
    "    use_fast = False\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "# Define the gen function\n",
    "def gen(model, prompt, max_new_tokens=1024):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    generation_config = GenerationConfig(\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.1,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Instruct: Summarize the following conversation.\n",
      "#Person1#: What does your sister look like, Mike?\n",
      "#Person2#: Well, she's tall and pretty.\n",
      "#Person1#: Is she like you?\n",
      "#Person2#: I suppose so. We're both friendly and easy-going.\n",
      "#Person1#: Is she as clever as you?\n",
      "#Person2#: No, she's not as clever as me.\n",
      "#Person1#: Big head!\n",
      "Output:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Mike is describing his sister to #Person1#.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Person 1 asks about Person 2's sister and describes her physical appearance, personality traits, and intelligence level in comparison to Person 2. Person 2 responds that their sister is tall and pretty but not as clever as them. Person 1 concludes by stating that their sister has a big head.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import set_seed\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "index = 51\n",
    "\n",
    "prompt = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "formatted_prompt = f\"Instruct: Summarize the following conversation.\\n{prompt}\\nOutput:\\n\"\n",
    "res = gen(original_model, formatted_prompt, 100,)\n",
    "\n",
    "output = res[0].split('Output:\\n')[1]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{formatted_prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample):\n",
    "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION_KEY = \"### Instruct: Summarize the below conversation.\"\n",
    "    RESPONSE_KEY = \"### Output:\"\n",
    "    END_KEY = \"### End\"\n",
    "\n",
    "    blurb = f\"\\n{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\"\n",
    "    input_context = f\"{sample['dialogue']}\" if sample[\"dialogue\"] else None\n",
    "    response = f\"{RESPONSE_KEY}\\n{sample[\"summary\"]}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "\n",
    "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
    "\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max length : {max_length}\")\n",
    "            break\n",
    "        if not max_length:\n",
    "            max_length = 1024\n",
    "            print(f\"Using default max length: {max_length}\")\n",
    "        return max_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    return tokenizer(\n",
    "        batch['text'],\n",
    "        max_length = max_length, \n",
    "        truncation = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def preprocess_dataset(tokenizer:AutoTokenizer, max_length : int, seed, dataset):\n",
    "    print(\"Preprocessing dataset......\")\n",
    "\n",
    "    dataset = dataset.map(create_prompt_formats)\n",
    "\n",
    "    preprocessing_function = partial(preprocess_batch, max_length = max_length, tokenizer = tokenizer)\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        preprocessing_function,\n",
    "        batched = True,\n",
    "        remove_columns = ['id', 'topic', 'dialogue', 'summary']\n",
    "    )\n",
    "\n",
    "    dataset = dataset.filter(lambda sample : len(sample[\"input_ids\"]) < max_length)\n",
    "\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default max length: 1024\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "max_length = get_max_length(original_model)\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1999/1999 [00:00<00:00, 17574.62 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1999/1999 [00:01<00:00, 1362.56 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1999/1999 [00:00<00:00, 7369.48 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 499/499 [00:00<00:00, 11636.59 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 499/499 [00:00<00:00, 1212.08 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 499/499 [00:00<00:00, 5958.45 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = preprocess_dataset(tokenizer, max_length, seed, dataset['train'])\n",
    "eval_dataset = preprocess_dataset(tokenizer, max_length, seed, dataset['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 20,971,520 || all params: 2,800,655,360 || trainable%: 0.7488\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "original_model = prepare_model_for_kbit_training(original_model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r = 32, #rank\n",
    "    lora_alpha = 32,\n",
    "    target_modules = [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"dense\"\n",
    "    ],\n",
    "    bias = \"none\",\n",
    "    lora_dropout = 0.05, #conventional\n",
    "    task_type = \"CAUSAL_LM\"\n",
    "\n",
    "    )\n",
    "original_model.gradient_checkpointing_enable()\n",
    "\n",
    "peft_model = get_peft_model(original_model, config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sharm\\phi-2-fine-tuning\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    warmup_steps = 3,\n",
    "    per_device_train_batch_size = 4,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    max_steps = 1000,\n",
    "    learning_rate = 2e-4,\n",
    "    optim = \"paged_adamw_8bit\",\n",
    "    logging_steps = 50,\n",
    "    logging_dir = \"./logs\",\n",
    "    save_strategy = \"steps\",\n",
    "    save_steps = 100,\n",
    "    evaluation_strategy = 'steps',\n",
    "    eval_steps = 100,\n",
    "    do_eval = True,\n",
    "    gradient_checkpointing = True,\n",
    "    report_to = \"none\",\n",
    "    overwrite_output_dir = True,\n",
    "    group_by_length = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "peft_model.config.use_cache = False\n",
    "\n",
    "peft_trainer = transformers.Trainer(\n",
    "    model = peft_model,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    args = peft_training_args,\n",
    "    data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm = False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â–Œ         | 50/1000 [06:49<1:47:06,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.477, 'grad_norm': 0.14942295849323273, 'learning_rate': 0.00019057171514543633, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 100/1000 [13:05<2:29:36,  9.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3076, 'grad_norm': 0.14999301731586456, 'learning_rate': 0.00018054162487462388, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 10%|â–ˆ         | 100/1000 [15:26<2:29:36,  9.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3367149829864502, 'eval_runtime': 140.4978, 'eval_samples_per_second': 3.552, 'eval_steps_per_second': 0.448, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–Œ        | 150/1000 [21:35<1:18:04,  5.51s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2937, 'grad_norm': 0.15707220137119293, 'learning_rate': 0.00017051153460381145, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 200/1000 [28:10<1:43:20,  7.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3065, 'grad_norm': 0.13852399587631226, 'learning_rate': 0.00016048144433299902, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 20%|â–ˆâ–ˆ        | 200/1000 [30:07<1:43:20,  7.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3244941234588623, 'eval_runtime': 116.9292, 'eval_samples_per_second': 4.268, 'eval_steps_per_second': 0.539, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–Œ       | 250/1000 [36:05<2:03:02,  9.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2449, 'grad_norm': 0.13445858657360077, 'learning_rate': 0.00015045135406218657, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 300/1000 [42:46<1:17:57,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2994, 'grad_norm': 0.15030372142791748, 'learning_rate': 0.00014042126379137414, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 300/1000 [44:44<1:17:57,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3230178356170654, 'eval_runtime': 118.149, 'eval_samples_per_second': 4.223, 'eval_steps_per_second': 0.533, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 350/1000 [51:09<1:46:06,  9.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2623, 'grad_norm': 0.15852603316307068, 'learning_rate': 0.00013039117352056168, 'epoch': 2.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 400/1000 [57:04<56:03,  5.61s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.241, 'grad_norm': 0.1846274584531784, 'learning_rate': 0.00012036108324974927, 'epoch': 3.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 400/1000 [59:02<56:03,  5.61s/it]c:\\Users\\sharm\\phi-2-fine-tuning\\.venv\\Lib\\site-packages\\peft\\utils\\other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /microsoft/phi-2/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000029309CD0410>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: c80bb420-c6ba-4264-9987-db58a681db02)') - silently ignoring the lookup for the file config.json in microsoft/phi-2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sharm\\phi-2-fine-tuning\\.venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:243: UserWarning: Could not find a config file in microsoft/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3211191892623901, 'eval_runtime': 117.9497, 'eval_samples_per_second': 4.231, 'eval_steps_per_second': 0.534, 'epoch': 3.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 450/1000 [1:05:38<1:11:35,  7.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2621, 'grad_norm': 0.16903765499591827, 'learning_rate': 0.00011033099297893681, 'epoch': 3.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 500/1000 [1:11:30<1:23:12,  9.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2086, 'grad_norm': 0.16520389914512634, 'learning_rate': 0.00010030090270812438, 'epoch': 4.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 500/1000 [1:13:28<1:23:12,  9.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3232418298721313, 'eval_runtime': 117.5302, 'eval_samples_per_second': 4.246, 'eval_steps_per_second': 0.536, 'epoch': 4.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 550/1000 [1:20:06<48:26,  6.46s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2554, 'grad_norm': 0.19089876115322113, 'learning_rate': 9.027081243731194e-05, 'epoch': 4.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 600/1000 [1:26:19<1:01:46,  9.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2118, 'grad_norm': 0.21383000910282135, 'learning_rate': 8.024072216649951e-05, 'epoch': 4.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 600/1000 [1:28:17<1:01:46,  9.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3225795030593872, 'eval_runtime': 117.5416, 'eval_samples_per_second': 4.245, 'eval_steps_per_second': 0.536, 'epoch': 4.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 650/1000 [1:34:17<32:29,  5.57s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1942, 'grad_norm': 0.22171078622341156, 'learning_rate': 7.021063189568707e-05, 'epoch': 5.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 700/1000 [1:40:55<39:40,  7.93s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2195, 'grad_norm': 0.2069230079650879, 'learning_rate': 6.018054162487463e-05, 'epoch': 5.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 700/1000 [1:42:53<39:40,  7.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3229470252990723, 'eval_runtime': 118.4158, 'eval_samples_per_second': 4.214, 'eval_steps_per_second': 0.532, 'epoch': 5.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 750/1000 [1:48:49<43:00, 10.32s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1663, 'grad_norm': 0.18576709926128387, 'learning_rate': 5.015045135406219e-05, 'epoch': 6.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 800/1000 [1:55:22<21:37,  6.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2166, 'grad_norm': 0.22938580811023712, 'learning_rate': 4.0120361083249755e-05, 'epoch': 6.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 800/1000 [1:57:20<21:37,  6.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3249585628509521, 'eval_runtime': 117.8948, 'eval_samples_per_second': 4.233, 'eval_steps_per_second': 0.534, 'epoch': 6.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 850/1000 [2:03:42<23:06,  9.25s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1719, 'grad_norm': 0.24068766832351685, 'learning_rate': 3.0090270812437316e-05, 'epoch': 6.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 900/1000 [2:09:42<09:01,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1688, 'grad_norm': 0.2611583173274994, 'learning_rate': 2.0060180541624878e-05, 'epoch': 7.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 900/1000 [2:11:40<09:01,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3280491828918457, 'eval_runtime': 118.1582, 'eval_samples_per_second': 4.223, 'eval_steps_per_second': 0.533, 'epoch': 7.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 950/1000 [2:18:17<06:13,  7.47s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.194, 'grad_norm': 0.2562006413936615, 'learning_rate': 1.0030090270812439e-05, 'epoch': 7.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [2:24:15<00:00, 11.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1244, 'grad_norm': 0.2056654542684555, 'learning_rate': 0.0, 'epoch': 8.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [2:26:13<00:00, 11.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3294334411621094, 'eval_runtime': 118.2628, 'eval_samples_per_second': 4.219, 'eval_steps_per_second': 0.533, 'epoch': 8.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [2:26:15<00:00,  8.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 8775.3003, 'train_samples_per_second': 1.823, 'train_steps_per_second': 0.114, 'train_loss': 1.2413030166625976, 'epoch': 8.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=1.2413030166625976, metrics={'train_runtime': 8775.3003, 'train_samples_per_second': 1.823, 'train_steps_per_second': 0.114, 'total_flos': 7.450303140421632e+16, 'train_loss': 1.2413030166625976, 'epoch': 8.016032064128256})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sharm\\phi-2-fine-tuning\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:46<00:00, 23.43s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "base_model_id = \"microsoft/phi-2\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    device_map = device_map,\n",
    "    quantization_config = bnb_config,\n",
    "    trust_remote_code = True,\n",
    "    use_auth_token = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token = True,\n",
    "    use_fast = False\n",
    "    )\n",
    "\n",
    "eval_tokenizer.pad_token = eval_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    r\"peft-dialogue-summary-training-1729186817\\checkpoint-1000\",\n",
    "    torch_dtype = torch.float16,\n",
    "    is_trainable = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Instruct: Summarize the following conversation.\n",
      "#Person1#: You're finally here! What took so long?\n",
      "#Person2#: I got stuck in traffic again. There was a terrible traffic jam near the Carrefour intersection.\n",
      "#Person1#: It's always rather congested down there during rush hour. Maybe you should try to find a different route to get home.\n",
      "#Person2#: I don't think it can be avoided, to be honest.\n",
      "#Person1#: perhaps it would be better if you started taking public transport system to work.\n",
      "#Person2#: I think it's something that I'll have to consider. The public transport system is pretty good.\n",
      "#Person1#: It would be better for the environment, too.\n",
      "#Person2#: I know. I feel bad about how much my car is adding to the pollution problem in this city.\n",
      "#Person1#: Taking the subway would be a lot less stressful than driving as well.\n",
      "#Person2#: The only problem is that I'm going to really miss having the freedom that you have with a car.\n",
      "#Person1#: Well, when it's nicer outside, you can start biking to work. That will give you just as much freedom as your car usually provides.\n",
      "#Person2#: That's true. I could certainly use the exercise!\n",
      "#Person1#: So, are you going to quit driving to work then?\n",
      "#Person2#: Yes, it's not good for me or for the environment.\n",
      "Output:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person2# complains to #Person1# about the traffic jam, #Person1# suggests quitting driving and taking public transportation instead.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PEFT MODEL:\n",
      "#Person2# got stuck in traffic and decides to take public transport system to work. #Person1# suggests #Person2# consider biking to work instead of driving since it's healthier and more environmentally friendly.\n",
      "\n",
      "#End of output#\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import set_seed\n",
    "set_seed(seed)\n",
    "\n",
    "index = 5\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n",
    "\n",
    "peft_model_res = gen(peft_model,prompt,100,)\n",
    "peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n",
    "#print(peft_model_output)\n",
    "prefix, success, result = peft_model_output.partition('###')\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL:\\n{prefix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.24s/it]\n"
     ]
    }
   ],
   "source": [
    "original_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n",
    "                                                      device_map='auto',\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True,\n",
    "                                                      use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Person1# needs #Person2# to take a dictation for him to send out an announcement restricting office communications to email and official memos, which will apply to both internal and external communications.\n",
      "\n",
      "### End of Output\n",
      "\n",
      "#Person1# thinks instant messaging wastes too much time. He asks #Person2# about its application. #Person1# reminds #Person2# to distribute the memo to all employees before 4pm.\n",
      "\n",
      "### End of Input\n",
      "\n",
      "#Person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Person1# asks Ms. Dawson to take a dictation for #Person1# to send out as an intra-office memorandum to all employees as a restriction on Instant Messaging. #Person2# suggests that some employees use Instant Messaging to communicate with their clients, so #Person1# insists it should apply to all communications. #Person2# confirms it and then continues taking the dictation.\n",
      "\n",
      "End of Output.\n",
      "\n",
      "### End of Text\n",
      "\n",
      "### Output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Person1# asks Ms. Dawson to take a dictation for #Person1#. #Person1# requests Ms. Dawson to write a memo about a new policy restricting communications to email and official memos. #Person1# thinks the new policy will waste time if used by employees. #Person1# tells Ms. Dawson to keep updating the memo and gives some other information.\n",
      "\n",
      "### End of Output\n",
      "\n",
      "### Instruction: Summarize the below conversation.\n",
      "\n",
      "#Person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Person1# suggests #Person2# take public transportation or bike instead of driving because #Person2#'s car contributes to pollution and stress. #Person2# agrees.\n",
      "\n",
      "End of output.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Person2# gets stuck in traffic and #Person1# suggests finding a new way to get home, like taking public transportation or biking. #Person2# decides to stop driving to work because it's not good for #Person2# or the environment.\n",
      "\n",
      "#Person3# joins the conversation.\n",
      "#Person3#: Why do you need to decide whether to stop driving or not?\n",
      "#Person1#: Because #Person2#'s car adds to the pollution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Person2# got stuck in traffic and #Person1# suggests #Person2# starts taking public transportation or biking to work instead of driving. #Person2# agrees and decides to stop driving to work.\n",
      "\n",
      "### End of Output\n",
      "\n",
      "#Person1#: You're finally here! What took so long?\n",
      "#Person2#: I got stuck in traffic again. There was a terrible traffic jam near the Carrefour intersection.\n",
      "#Person1#: It's\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masha tells Kate that her parents are getting divorced after two-month separation. They've worked out everything except the divorce itself.\n",
      "\n",
      "#Person1#: That's the change from all the back stepping we usually hear about. Well, I still can't believe it, Masha and Hero, the perfect couple. When would they divorce be final?\n",
      "\n",
      "#Person2#: Early in the New Year I guess.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masha tells Kate her parents Masha and Hero will divorce after two months of separation.\n",
      "\n",
      "#Person1#'s parents decided to have a quiet divorce with no fighting over details. #Person1# is surprised.\n",
      "\n",
      "#Person1# tells Kate that the divorce will be finalized early in the new year.\n",
      "\n",
      "###End of Output.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masha tells Kate that her husband Hero is divorcing her after two months of separation without any conflict over custody of their children.\n",
      "\n",
      "End of output.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brian invites #Person1# to have a dance at his birthday party. They compliment each other after having a drink together.\n",
      "\n",
      "### End of Output\n",
      "\n",
      "The conversation between Brian and #Person1# describes their compliments on each other after having a drink together.\n",
      "\n",
      "### End of Input\n",
      "\n",
      "### \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>peft_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>Ms. Dawson instructed Mr. Johnson to summarize...</td>\n",
       "      <td>#Person1# needs #Person2# to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting tim...</td>\n",
       "      <td>Ms. Dawson instructed Person 2 to summarize th...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
       "      <td>Ms. Dawson informs all staff members about a n...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam....</td>\n",
       "      <td>\\nThe conversation between Person 1 and Person...</td>\n",
       "      <td>#Person1# suggests #Person2# take public trans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
       "      <td># Person1 and #Person2# are discussing whether...</td>\n",
       "      <td>#Person2# gets stuck in traffic and #Person1# ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person2# complains to #Person1# about the tra...</td>\n",
       "      <td>Person 2 has been caught in traffic, and they ...</td>\n",
       "      <td>#Person2# got stuck in traffic and #Person1# s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n",
       "      <td>Kate informed Person2 that Masha and Hero have...</td>\n",
       "      <td>Masha tells Kate that her parents are getting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n",
       "      <td>Kate informs Person2 that Masha and Hero have ...</td>\n",
       "      <td>Masha tells Kate her parents Masha and Hero wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person1# and Kate talk about the divorce betw...</td>\n",
       "      <td>Kate informs Person2 that Masha and Hero have ...</td>\n",
       "      <td>Masha tells Kate that her husband Hero is divo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# and Brian are at the birthday party ...</td>\n",
       "      <td>Brian thanked his friend for remembering his b...</td>\n",
       "      <td>Brian invites #Person1# to have a dance at his...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            human_baseline_summaries  \\\n",
       "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1  In order to prevent employees from wasting tim...   \n",
       "2  Ms. Dawson takes a dictation for #Person1# abo...   \n",
       "3  #Person2# arrives late because of traffic jam....   \n",
       "4  #Person2# decides to follow #Person1#'s sugges...   \n",
       "5  #Person2# complains to #Person1# about the tra...   \n",
       "6  #Person1# tells Kate that Masha and Hero get d...   \n",
       "7  #Person1# tells Kate that Masha and Hero are g...   \n",
       "8  #Person1# and Kate talk about the divorce betw...   \n",
       "9  #Person1# and Brian are at the birthday party ...   \n",
       "\n",
       "                            original_model_summaries  \\\n",
       "0  Ms. Dawson instructed Mr. Johnson to summarize...   \n",
       "1  Ms. Dawson instructed Person 2 to summarize th...   \n",
       "2  Ms. Dawson informs all staff members about a n...   \n",
       "3  \\nThe conversation between Person 1 and Person...   \n",
       "4  # Person1 and #Person2# are discussing whether...   \n",
       "5  Person 2 has been caught in traffic, and they ...   \n",
       "6  Kate informed Person2 that Masha and Hero have...   \n",
       "7  Kate informs Person2 that Masha and Hero have ...   \n",
       "8  Kate informs Person2 that Masha and Hero have ...   \n",
       "9  Brian thanked his friend for remembering his b...   \n",
       "\n",
       "                                peft_model_summaries  \n",
       "0  #Person1# needs #Person2# to take a dictation ...  \n",
       "1  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "2  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "3  #Person1# suggests #Person2# take public trans...  \n",
       "4  #Person2# gets stuck in traffic and #Person1# ...  \n",
       "5  #Person2# got stuck in traffic and #Person1# s...  \n",
       "6  Masha tells Kate that her parents are getting ...  \n",
       "7  Masha tells Kate her parents Masha and Hero wi...  \n",
       "8  Masha tells Kate that her husband Hero is divo...  \n",
       "9  Brian invites #Person1# to have a dance at his...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "for idx, dialogue in enumerate(dialogues):\n",
    "    human_baseline_text_output = human_baseline_summaries[idx]\n",
    "    prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n",
    "    \n",
    "    original_model_res = gen(original_model,prompt,100,)\n",
    "    original_model_text_output = original_model_res[0].split('Output:\\n')[1]\n",
    "    \n",
    "    peft_model_res = gen(peft_model,prompt,100,)\n",
    "    peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n",
    "    print(peft_model_output)\n",
    "    peft_model_text_output, success, result = peft_model_output.partition('###')\n",
    "\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "    peft_model_summaries.append(peft_model_text_output)\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n",
    " \n",
    "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.25399343006628494, 'rouge2': 0.07524937651267982, 'rougeL': 0.17981781715544354, 'rougeLsum': 0.17890899386281736}\n",
      "PEFT MODEL:\n",
      "{'rouge1': 0.32593791675457295, 'rouge2': 0.0989859486679662, 'rougeL': 0.22189633024448713, 'rougeLsum': 0.2443493201016658}\n",
      "Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\n",
      "rouge1: 7.19%\n",
      "rouge2: 2.37%\n",
      "rougeL: 4.21%\n",
      "rougeLsum: 6.54%\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)\n",
    "\n",
    "print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n",
    "\n",
    "improvement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\n",
    "for key, value in zip(peft_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
